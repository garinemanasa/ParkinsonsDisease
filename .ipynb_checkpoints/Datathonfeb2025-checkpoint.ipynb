{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ab85b-3b7e-49b1-97f4-33a1e2f36cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd8b3064-934a-4ff9-af0f-af0c4e8b4e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement dict_handling (from versions: none)\n",
      "ERROR: No matching distribution found for dict_handling\n"
     ]
    }
   ],
   "source": [
    "pip install dict_handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b69163-400e-4b88-b77b-f55b3747ca2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fadf2e-390f-43a4-92f0-d462c5f0e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------data analysis-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39d41d3-ff4f-4a29-898a-b823cf1c49cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'Documents/GitHub/ParkinsonsDisease/pads-parkinsons-disease-smartwatch-dataset'\n",
    "patient_dir = 'Documents/GitHub/ParkinsonsDisease/pads-parkinsons-disease-smartwatch-dataset/patients'\n",
    "movement_dir = 'Documents/GitHub/ParkinsonsDisease/pads-parkinsons-disease-smartwatch-dataset/movement'\n",
    "questionnaire_dir = 'Documents/GitHub/ParkinsonsDisease/pads-parkinsons-disease-smartwatch-dataset/questionnaire'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe04a7b4-8e3e-4341-9823-71f570cbfbe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=['dataset_dir','patient_dir','movement_dir','questionnaire_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20925af-d28a-466e-94a4-175fdbaf59c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b38f6c-9027-45bd-b4e2-4ebdd868427a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0950e694-f5a5-4a94-8513-c7ddb2671b14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flatten_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"from dict_handling\"\"\"\u001b[39;00m \n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflatten_dict\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_all_files\u001b[39m(path, dataframe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Load all .json files from the defined directory and return all the loaded meta data.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m        Whether to flatten the meta data into dataframes.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flatten_dict'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dict_handling import flatten_dict\n",
    "\n",
    "\n",
    "def load_all_files(path, dataframe=True):\n",
    "    \"\"\"\n",
    "    Load all .json files from the defined directory and return all the loaded meta data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the directory holding the .json files.\n",
    "    dataframe : bool, default = True\n",
    "        Whether to flatten the meta data into dataframes.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    search_space = glob(path + '*json')\n",
    "    search_space.sort()\n",
    "    for f_name in search_space:\n",
    "        with open(f_name, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if dataframe:\n",
    "                data = flatten_dict(data)\n",
    "                data = pd.DataFrame(data)\n",
    "            data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def get_data_from_txt_file(path, n_channels):\n",
    "    record = np.loadtxt(path, dtype=np.float32, delimiter=\",\")\n",
    "    return record\n",
    "\n",
    "\n",
    "def get_data_from_questionnaire_response(meta_file):\n",
    "    data = meta_file['answer'].values\n",
    "    channels = (meta_file['questionnaire_name'] + '_' + meta_file['link_id']).values\n",
    "    return data, channels\n",
    "\n",
    "\n",
    "def get_data_from_observation(path, meta_file):\n",
    "    all_records = []\n",
    "    all_channels = []\n",
    "    min_rows = meta_file['rows'].min()\n",
    "    for idx, meta_item in meta_file.iterrows():\n",
    "        n_splits = meta_item['rows'] // min_rows\n",
    "\n",
    "        file_path = meta_item['file_name']\n",
    "        record = get_data_from_txt_file(path + file_path, len(meta_item['channels']))\n",
    "        record = np.swapaxes(record, 0, 1)\n",
    "        channels = ['_'.join([meta_item['device_location'], channel]) for channel in meta_item['channels']]\n",
    "\n",
    "        # Re-organize the raw data so that each record has the same length and all records fit into one matrix\n",
    "        step = record.shape[1] // n_splits\n",
    "        if n_splits > 1:\n",
    "            new_record = []\n",
    "            for n in range(0, record.shape[1], step):\n",
    "                new_record.append(record[:, n:n+step])\n",
    "            record = np.concatenate(new_record, axis=0)\n",
    "            new_channels = []\n",
    "            for n in range(n_splits):\n",
    "                for channel in channels:\n",
    "                    new_channels.append(f'{meta_item[\"record_name\"]}{n+1}_{channel}')\n",
    "            channels = new_channels\n",
    "        else:\n",
    "            channels = ['_'.join([meta_item['record_name'], channel]) for channel in channels]\n",
    "        all_records.append(record)\n",
    "        all_channels.extend(channels)\n",
    "\n",
    "    all_records = np.concatenate(all_records, axis=0)\n",
    "\n",
    "    return all_records, all_channels\n",
    "\n",
    "\n",
    "def get_data(path):\n",
    "    data_list = []\n",
    "    channels_list = []\n",
    "    meta_list = load_all_files(path, dataframe=True)\n",
    "    for meta_file in meta_list:\n",
    "        if meta_file['resource_type'].iloc[0] == 'questionnaire_response':\n",
    "            data, channels = get_data_from_questionnaire_response(meta_file)\n",
    "        elif meta_file['resource_type'].iloc[0] == 'observation':\n",
    "            data, channels = get_data_from_observation(path, meta_file)\n",
    "        else:\n",
    "            raise Exception(f'The \"resource_type\" {meta_file[\"resource_type\"].iloc[0]} could not be loaded.')\n",
    "        data_list.append(data)\n",
    "        channels_list.append(channels)\n",
    "    return np.array(data_list, dtype=np.float32), channels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674c5c1-bb1c-4d63-a05e-0217274a6714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
